{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubham62025865/shubham1/blob/main/Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent"
      ],
      "metadata": {
        "id": "rkG4ubbSDEI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks."
      ],
      "metadata": {
        "id": "2I7c_e54DMlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "7oi9G901DrOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the 3-dimensional graph below in the context of a cost function. Our goal is to move from the mountain in the top right corner (high cost) to the dark blue sea in the bottom left (low cost). The arrows represent the direction of steepest descent (negative gradient) from any given point–the direction that decreases the cost function as quickly as possible."
      ],
      "metadata": {
        "id": "67347A3bD-gD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img width = 600 src = \"https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent.png\" />"
      ],
      "metadata": {
        "id": "uIE0HT5aEDmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img width = 500 img = \"https://miro.medium.com/max/1400/1*IfVAzJJWIrCw0saOZAxpzA.png\" />"
      ],
      "metadata": {
        "id": "QY25gZ-FEgax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv()"
      ],
      "metadata": {
        "id": "ldTseNnnJ4Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting at the top of the mountain, we take our first step downhill in the direction specified by the negative gradient. Next we recalculate the negative gradient (passing in the coordinates of our new point) and take another step in the direction it specifies. We continue this process iteratively until we get to the bottom of our graph, or to a point where we can no longer move downhill–a local minimum."
      ],
      "metadata": {
        "id": "fXc3B8i_FRZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img width = 400 src = https://149695847.v2.pressablecdn.com/wp-content/uploads/2022/07/image-99-1300x1257.png />"
      ],
      "metadata": {
        "id": "UdJBA28BFy0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost Function\n",
        "\n",
        "The primary set-up for learning neural networks is to define a cost function (also known as a loss function) that measures how well the network predicts outputs on the test set. The goal is to then find a set of weights and biases that minimizes the cost. One common function that is often used is the mean squared error, which measures the difference between the actual value of y and the estimated value of y (the prediction). The equation of the below regression line is hθ(x) = θ + θ1x, which has only two parameters: weight (θ1)and bias (θ0).\n",
        "\n",
        "<img width = 600 src = \"https://miro.medium.com/max/1400/1*ool361dWI61RMDyUAmalmA.png\" />"
      ],
      "metadata": {
        "id": "jpKMLYUpGXFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Difference between cost function and loss function\n",
        "\n",
        "The loss function is to capture the difference between the actual and predicted values for a single record whereas cost functions aggregate the difference for the entire training dataset."
      ],
      "metadata": {
        "id": "nJeUyLeuHCfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to Minimise the Cost Function\n",
        "\n",
        "Our goal is to move from the mountain in the top right corner (high cost) to the dark blue sea in the bottom left (low cost). In order to get the lowest error value, we need to adjust the weights ‘θ0’ and ‘θ1’ to reach the smallest possible error. This is because the result of a lower error between the actual and the predicted values means the algorithm has done a good job in learning. Gradient descent is an efficient optimization algorithm that attempts to find a local or global minimum of a function.\n",
        "\n",
        "\n",
        "<img width = 600 src= \"https://miro.medium.com/max/1400/1*IfVAzJJWIrCw0saOZAxpzA.png\" />"
      ],
      "metadata": {
        "id": "cbbT9HQmJDrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating gradient descent\n",
        "\n",
        "Gradient Descent runs iteratively to find the optimal values of the parameters corresponding to the minimum value of the given cost function, using calculus. Mathematically, the technique of the ‘derivative’ is extremely important to minimise the cost function because it helps get the minimum point. The derivative is a concept from calculus and refers to the slope of the function at a given point. We need to know the slope so that we know the direction (sign) to move the coefficient values in order to get a lower cost on the next iteration.\n",
        "\n",
        "<img width = 600 src = \"https://miro.medium.com/max/1400/1*tQTcGTLZqnI5rp3JYO_4NA.png\" />"
      ],
      "metadata": {
        "id": "wiOKdWyrJrGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The derivative of a function (in our case, J(θ)) on each parameter (in our case weight θ) tells us the sensitivity of the function with respect to that variable or how changing the variable impacts the function value. Gradient descent, therefore, enables the learning process to make corrective updates to the learned estimates that move the model toward an optimal combination of parameters (θ). The cost is calculated for a machine learning algorithm over the entire training dataset for each iteration of the gradient descent algorithm. In Gradient Descent, one iteration of the algorithm is called one batch, which denotes the total number of samples from a dataset that is used for calculating the gradient for each iteration."
      ],
      "metadata": {
        "id": "FEwWhDY3K3wi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The step of the derivation\n",
        "\n",
        "It would be better if you have some basic understanding of calculus because the technique of the partial derivative and the chain rule is being applied in this case.\n",
        "\n",
        "<img width = 700 src = https://miro.medium.com/max/1400/1*mY-QXHlRYhZ3vp-93opAIA.png />"
      ],
      "metadata": {
        "id": "qnpBjAazM1ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To solve for the gradient, we iterate through our data points using our new weight ‘θ0’ and bias ‘θ1’ values and compute the partial derivatives. This new gradient tells us the slope of our cost function at our current position (current parameter values) and the direction we should move to update our parameters. The size of our update is controlled by the learning rate."
      ],
      "metadata": {
        "id": "96cQmoa1NcVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning rate\n",
        "\n",
        "\n",
        "The size of these steps is called the learning rate (α) that gives us some additional control over how large of steps we make. With a large learning rate, we can cover more ground each step, but we risk overshooting the lowest point since the slope of the hill is constantly changing. With a very low learning rate, we can confidently move in the direction of the negative gradient since we are recalculating it so frequently. A low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom. The most commonly used rates are: 0.001, 0.003, 0.01, 0.03, 0.1, 0.3.\n",
        "\n",
        "<img width = 600 src = https://miro.medium.com/max/1400/1*GTRi-Y2doXbbrc4lGJYH-w.png />"
      ],
      "metadata": {
        "id": "oMx_e7UbNiu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For practice - https://uclaacm.github.io/gradient-descent-visualiser/\n",
        "\n",
        "And\n",
        "\n",
        "https://blog.skz.dev/gradient-descent\n",
        "\n",
        "And this - https://developers.google.com/machine-learning/crash-course/fitter/graph"
      ],
      "metadata": {
        "id": "KZ8yBOf_OdVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating batch gradient desent"
      ],
      "metadata": {
        "id": "H1LkR_WFSRWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "8Fa3F2Q3SQYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.randn(10,1)\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg9PiEKuSdj3",
        "outputId": "aed01932-8d81-4ead-e7d5-4a3e55e8c900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.2229689 ],\n",
              "       [ 0.67857281],\n",
              "       [ 0.47457011],\n",
              "       [ 1.65577279],\n",
              "       [ 0.08845082],\n",
              "       [-0.36421057],\n",
              "       [ 0.34498585],\n",
              "       [-0.58182895],\n",
              "       [-1.9332853 ],\n",
              "       [-0.06802657]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = 2*X + 6\n",
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4ikgUOnS-b1",
        "outputId": "0bdaa3c9-4a50-417e-aaa8-bbbb42789e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.5540622 ],\n",
              "       [7.35714563],\n",
              "       [6.94914023],\n",
              "       [9.31154559],\n",
              "       [6.17690164],\n",
              "       [5.27157887],\n",
              "       [6.6899717 ],\n",
              "       [4.8363421 ],\n",
              "       [2.1334294 ],\n",
              "       [5.86394686]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# w,b, learning rate, loss func derivative"
      ],
      "metadata": {
        "id": "2r_xpR_QL4XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b0 = 0\n",
        "b1 = 0\n",
        "learning_rate = 0.01"
      ],
      "metadata": {
        "id": "syrPBX5oL4PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ayQf3DbPRfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def gd(X, Y, epochs = 100, learning_rate = 0.01):\n",
        "\n",
        "  b0 = 0\n",
        "  b1 = 0\n",
        "\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    dedb0 = 0\n",
        "    dedb1 = 0\n",
        "\n",
        "\n",
        "    for i in range(len(X)):\n",
        "      dedb0 = dedb0 + b0 + b1 * X[i] - Y[i]\n",
        "      dedb1 = dedb1 + (b0 + b1 * X[i] - Y[i]) * (X[i])\n",
        "\n",
        "    b0 = b0 - learning_rate * dedb0 * 2 / float(len(X))\n",
        "    b1 = b1 - learning_rate * dedb1 * 2/ float(len(X))\n",
        "\n",
        "\n",
        "  return b0, b1\n",
        "\n"
      ],
      "metadata": {
        "id": "oWpSVJn7Iktl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gd(X, Y, epochs = 35, learning_rate = 0.1)"
      ],
      "metadata": {
        "id": "ITUpxvLrIkk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2449e171-81fb-4e4f-b9ad-d05d02a9c049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([5.99554936]), array([1.99503569]))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stochastic gradient descent (SGD)"
      ],
      "metadata": {
        "id": "g_n80vqt3CV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, there is a disadvantage of applying a typical Gradient Descent optimization technique in our dataset. It becomes computationally very expensive to perform because we have to use all of the one million samples for completing one iteration, and it has to be done for every iteration until the minimum point is reached. This problem can be solved by Stochastic Gradient Descent.\n",
        "\n",
        "The word ‘stochastic’ means a system or a process that is linked with a random probability. Stochastic gradient descent uses this idea to speed up the process of performing gradient descent. Hence, unlike the typical Gradient Descent optimization, instead of using the whole data set for each iteration, we are able to use the cost gradient of only 1 example at each iteration (details are shown in the graph below). Even though using the whole dataset is really useful for getting to the minima in a less noisy or less random manner, the problem arises when our datasets get really large."
      ],
      "metadata": {
        "id": "cw6ZmhiV3NtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img width = 600 src= https://miro.medium.com/max/1400/1*GwEtRMIIU7StzlUv4ysTWg.png />"
      ],
      "metadata": {
        "id": "CilNQBLe3VZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two main differences are that the stochastic gradient descent method helps us avoid the problem where we find those local extremities or local minimums rather than the overall global minimum.\n",
        "\n",
        "As mentioned, the stochastic gradient descent method is doing one iteration or one row at a time, and therefore, the fluctuations are much higher than the batch gradient descent."
      ],
      "metadata": {
        "id": "7Hs-n5e13p3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Three variants of gradient descent algorithm** -\n",
        "\n",
        "- Batch gradient descent (BGD): calculate the error for each example in the training dataset, but only updates the model after all training examples have been evaluated.\n",
        "\n",
        "- Stochastic gradient descent (SGD): calculate the error and updates the model for each examplein the training dataset.\n",
        "\n",
        "- Mini-Batch gradient descent: split the training dataset into small batches that are used to calculate model error and updated model coefficients. (the most common implementation of gradient descent used in the field of deep learning)"
      ],
      "metadata": {
        "id": "7TQ0KL1-3u-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BGD\n",
        "**Advantages:**\n",
        "\n",
        "- Easy computation.\n",
        "- Easy to implement.\n",
        "- Easy to understand.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "- May trap at local minima.\n",
        "- Weights are changed after calculating the gradient on the whole dataset. So, if the dataset is too large then this may take years to converge to the minima.\n",
        "\n",
        "- Requires large memory to calculate the gradient on the whole dataset."
      ],
      "metadata": {
        "id": "zPFgQO8gtLWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SGD\n",
        "\n",
        "<img width = 600 src = https://static.wixstatic.com/media/3eee0b_2f20c4c9902844718350e189e57fd909~mv2.png/v1/fill/w_740,h_290,al_c,q_90,usm_0.66_1.00_0.01/3eee0b_2f20c4c9902844718350e189e57fd909~mv2.webp />\n",
        "\n",
        "\n",
        "**Advantage:**\n",
        "\n",
        "- Memory requirement is less compared to the GD algorithm as the derivative is computed taking only 1 point at once.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "- The time required to complete 1 epoch is large compared to the GD algorithm.\n",
        "- Takes a long time to converge.\n",
        "- May stuck at local minima."
      ],
      "metadata": {
        "id": "NjFc2A9TtnKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MBGD\n",
        "\n",
        "<img width = 600 src = https://static.wixstatic.com/media/3eee0b_afe86f0d655d4b218f002ce82c1c25ac~mv2.png/v1/fill/w_740,h_190,al_c,q_90,usm_0.66_1.00_0.01/3eee0b_afe86f0d655d4b218f002ce82c1c25ac~mv2.webp />\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "- Less time complexity to converge compared to standard SGD algorithm.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "- The update of MB-SGD is much noisy compared to the update of the GD algorithm.\n",
        "- Take a longer time to converge than the GD algorithm.\n",
        "- May get stuck at local minima."
      ],
      "metadata": {
        "id": "18wk0FFBuM94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(X, y, learning_rate, epochs, batch_size):\n",
        "    # Add bias term to the feature matrix\n",
        "    X = np.concatenate((np.ones((len(X),1)), X), axis = 1)\n",
        "\n",
        "    # Initialize weights with zeros\n",
        "    num_features = X.shape[1]\n",
        "    weights = np.zeros(num_features)\n",
        "\n",
        "    # Perform stochastic gradient descent\n",
        "    num_samples = len(y)\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle the data\n",
        "        indices = np.random.permutation(num_samples)\n",
        "        X_shuffled = X[indices]\n",
        "        y_shuffled = y[indices]\n",
        "\n",
        "        # Iterate over mini-batches\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            X_batch = X_shuffled[i:i+batch_size]\n",
        "            y_batch = y_shuffled[i:i+batch_size]\n",
        "\n",
        "            # Calculate the predicted values\n",
        "            y_pred = np.dot(X_batch, weights)\n",
        "\n",
        "            # Calculate the gradient\n",
        "            gradient = np.dot(X_batch.T, y_pred - y_batch) / len(y_batch)\n",
        "\n",
        "            # Update the weights\n",
        "            weights -= learning_rate * gradient\n",
        "\n",
        "    return weights"
      ],
      "metadata": {
        "id": "RaF3p5cEtK58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "num_epochs = 100\n",
        "batch_size = 10\n",
        "\n",
        "coefficients = sgd(X, Y.flatten(), learning_rate, num_epochs, batch_size)\n",
        "print(\"coefficients :\", coefficients)"
      ],
      "metadata": {
        "id": "WLvSM5napPIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# The data has already been sorted into training and test sets for us\n",
        "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "Z5qiuY-Jm4kO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b9aa7fa-69c1-4d64-ffa7-4704ded03643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of our data\n",
        "train_data.shape, train_labels.shape, test_data.shape, test_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EjJPbimdy06",
        "outputId": "0f8908b8-bc33-40df-d690-db3acc893a82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide train and test images by the maximum value (normalize it)\n",
        "train_data = train_data / 255.0\n",
        "test_data = test_data / 255.0\n",
        "\n",
        "# Check the min and max values of the training data\n",
        "train_data.min(), train_data.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmIMoRRZd4Uc",
        "outputId": "881c9836-a6d4-4adc-855c-d7f7ec71ed6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "60000 / 20000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRfjOau-WmIC",
        "outputId": "7267bfee-d083-48b5-8d27-88fb96dc659f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a mini batch gradient desent nn with batch size 17000"
      ],
      "metadata": {
        "id": "Jt3xCfDeUhRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "# build a neural network\n",
        "model_1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape = (28,28)),\n",
        "    tf.keras.layers.Dense(100, activation = \"relu\"),\n",
        "    tf.keras.layers.Dense(100, activation = \"relu\"),\n",
        "    tf.keras.layers.Dense(50, activation = \"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation = \"softmax\"),\n",
        "])\n",
        "\n",
        "# compile model\n",
        "\n",
        "model_1.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                metrics = [\"accuracy\"])\n",
        "\n",
        "# fit model\n",
        "\n",
        "history = model_1.fit(train_data, train_labels,\n",
        "                      validation_data = (test_data, test_labels),\n",
        "                      epochs = 20, batch_size = 40000)\n"
      ],
      "metadata": {
        "id": "Uu8w16-rgKJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d69955bb-c9a8-43a0-af09-00ebe1481e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2/2 [==============================] - 1s 559ms/step - loss: 2.3549 - accuracy: 0.1641 - val_loss: 1.9673 - val_accuracy: 0.3880\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 1s 395ms/step - loss: 1.9237 - accuracy: 0.4051 - val_loss: 1.7728 - val_accuracy: 0.4145\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 1.7040 - accuracy: 0.4490 - val_loss: 1.4804 - val_accuracy: 0.4893\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 1s 216ms/step - loss: 1.4911 - accuracy: 0.5014 - val_loss: 1.4380 - val_accuracy: 0.4919\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 1.3943 - accuracy: 0.5388 - val_loss: 1.2012 - val_accuracy: 0.6099\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 1s 258ms/step - loss: 1.1841 - accuracy: 0.6266 - val_loss: 1.0995 - val_accuracy: 0.6453\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 1.0869 - accuracy: 0.6494 - val_loss: 1.0207 - val_accuracy: 0.6578\n",
            "Epoch 8/20\n",
            "2/2 [==============================] - 1s 241ms/step - loss: 1.0042 - accuracy: 0.6652 - val_loss: 0.9973 - val_accuracy: 0.6496\n",
            "Epoch 9/20\n",
            "2/2 [==============================] - 1s 229ms/step - loss: 0.9917 - accuracy: 0.6607 - val_loss: 0.9653 - val_accuracy: 0.6510\n",
            "Epoch 10/20\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.9534 - accuracy: 0.6620 - val_loss: 0.9548 - val_accuracy: 0.6338\n",
            "Epoch 11/20\n",
            "2/2 [==============================] - 1s 243ms/step - loss: 0.9175 - accuracy: 0.6672 - val_loss: 0.8568 - val_accuracy: 0.6817\n",
            "Epoch 12/20\n",
            "2/2 [==============================] - 1s 258ms/step - loss: 0.8417 - accuracy: 0.7015 - val_loss: 0.8990 - val_accuracy: 0.6571\n",
            "Epoch 13/20\n",
            "2/2 [==============================] - 1s 249ms/step - loss: 0.8654 - accuracy: 0.6860 - val_loss: 0.8408 - val_accuracy: 0.6984\n",
            "Epoch 14/20\n",
            "2/2 [==============================] - 1s 261ms/step - loss: 0.8312 - accuracy: 0.7083 - val_loss: 0.9629 - val_accuracy: 0.6794\n",
            "Epoch 15/20\n",
            "2/2 [==============================] - 1s 237ms/step - loss: 0.8919 - accuracy: 0.7021 - val_loss: 0.7583 - val_accuracy: 0.7403\n",
            "Epoch 16/20\n",
            "2/2 [==============================] - 1s 237ms/step - loss: 0.7349 - accuracy: 0.7517 - val_loss: 0.7284 - val_accuracy: 0.7487\n",
            "Epoch 17/20\n",
            "2/2 [==============================] - 1s 252ms/step - loss: 0.7107 - accuracy: 0.7551 - val_loss: 0.7451 - val_accuracy: 0.7285\n",
            "Epoch 18/20\n",
            "2/2 [==============================] - 1s 244ms/step - loss: 0.7469 - accuracy: 0.7258 - val_loss: 0.7564 - val_accuracy: 0.7279\n",
            "Epoch 19/20\n",
            "2/2 [==============================] - 1s 236ms/step - loss: 0.7434 - accuracy: 0.7309 - val_loss: 0.7816 - val_accuracy: 0.7224\n",
            "Epoch 20/20\n",
            "2/2 [==============================] - 1s 342ms/step - loss: 0.7519 - accuracy: 0.7311 - val_loss: 0.7163 - val_accuracy: 0.7439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "id": "uhKYXNiWgKGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45000a30-3762-47c7-bda5-6d4bed277934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_3 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 100)               78500     \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 94,160\n",
            "Trainable params: 94,160\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QxQFSlWNgKD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BnEDt_EXgJ36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "id": "jM8A6A13gJ1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0404b572-b79d-4957-f53b-a0a13b2e4d7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "# create model\n",
        "model_1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape = (28,28)),\n",
        "    tf.keras.layers.Dense(100, activation = \"relu\"),\n",
        "    tf.keras.layers.Dense(100, activation = \"relu\"),\n",
        "    tf.keras.layers.Dense(50, activation = \"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation = \"softmax\")\n",
        "])\n",
        "\n",
        "# compile model\n",
        "\n",
        "model_1.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                metrics = [\"accuracy\"])\n",
        "\n",
        "# fit model\n",
        "\n",
        "history = model_1.fit(train_data, train_labels,\n",
        "                      epochs = 10, validation_data = (test_data, test_labels),\n",
        "                      batch_size = 5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfma3gM3eOsC",
        "outputId": "f717f702-6442-4db2-937d-c0073a14156a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "12/12 [==============================] - 1s 63ms/step - loss: 1.7686 - accuracy: 0.4873 - val_loss: 1.1658 - val_accuracy: 0.6569\n",
            "Epoch 2/10\n",
            "12/12 [==============================] - 1s 54ms/step - loss: 0.8891 - accuracy: 0.7088 - val_loss: 0.7248 - val_accuracy: 0.7384\n",
            "Epoch 3/10\n",
            "12/12 [==============================] - 1s 50ms/step - loss: 0.6432 - accuracy: 0.7750 - val_loss: 0.6074 - val_accuracy: 0.7897\n",
            "Epoch 4/10\n",
            "12/12 [==============================] - 1s 53ms/step - loss: 0.5510 - accuracy: 0.8111 - val_loss: 0.5467 - val_accuracy: 0.8096\n",
            "Epoch 5/10\n",
            "12/12 [==============================] - 1s 50ms/step - loss: 0.4995 - accuracy: 0.8290 - val_loss: 0.5127 - val_accuracy: 0.8234\n",
            "Epoch 6/10\n",
            "12/12 [==============================] - 1s 52ms/step - loss: 0.4680 - accuracy: 0.8391 - val_loss: 0.4833 - val_accuracy: 0.8317\n",
            "Epoch 7/10\n",
            "12/12 [==============================] - 1s 47ms/step - loss: 0.4459 - accuracy: 0.8462 - val_loss: 0.4661 - val_accuracy: 0.8353\n",
            "Epoch 8/10\n",
            "12/12 [==============================] - 1s 53ms/step - loss: 0.4286 - accuracy: 0.8510 - val_loss: 0.4523 - val_accuracy: 0.8415\n",
            "Epoch 9/10\n",
            "12/12 [==============================] - 1s 78ms/step - loss: 0.4160 - accuracy: 0.8551 - val_loss: 0.4403 - val_accuracy: 0.8440\n",
            "Epoch 10/10\n",
            "12/12 [==============================] - 1s 81ms/step - loss: 0.4036 - accuracy: 0.8586 - val_loss: 0.4379 - val_accuracy: 0.8474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contour plots\n",
        "\n",
        "<img src = https://i2.wp.com/www.adeveloperdiary.com/wp-content/uploads/2018/11/How-to-visualize-Gradient-Descent-using-Contour-plot-in-Python-adeveloperdiary.com-1.jpg />\n",
        "\n",
        "\n",
        "-----\n",
        "Contour Plot is like a 3D surface plot, where the 3rd dimension (Z) gets plotted as constant slices (contour) on a 2 Dimensional surface. The left plot at the picture below shows a 3D plot and the right one is the Contour plot of the same 3D plot.\n",
        "\n",
        " You can see how the 3rd dimension (Y here) has been converted to contours of colors ( and lines ). The important part is, the value of Y is always same across the contour line for all the values of X1 & X2.\n",
        "\n",
        "\n",
        "For more info - https://www.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function/visualizing-scalar-valued-functions/v/contour-plots"
      ],
      "metadata": {
        "id": "gQBtOocxnKUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://emiliendupont.github.io/2018/01/24/optimization-visualization/"
      ],
      "metadata": {
        "id": "bbama1OjMOuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c"
      ],
      "metadata": {
        "id": "h4Yn7lqGMcrV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w_gpjPiQMeQt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}